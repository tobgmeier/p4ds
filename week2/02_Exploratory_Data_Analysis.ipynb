{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Exploratory Data Analysis (EDA) on the Kings County Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.9 or later is installed (ideally I would generatlly recommend Python 3.10), as well as Scikit-Learn ≥ 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.9 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 9)\n",
    "\n",
    "# Scikit-Learn ≥1.0 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"1.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Precision options\n",
    "np.set_printoptions(precision=2)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "\n",
    "# Statistical analysis and testing\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the Data\n",
    "\n",
    "First of all let's import the data from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\n",
    "    \"../datasets/kings_county_house_data.csv\",\n",
    "    dtype={'zipcode': str}   # US ZIP codes look like numbers but we want to treat them like strings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an overall idea of the fields available using the `DataFrame.info()` and `DataFrame.describe()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description of the features:\n",
    "\n",
    "Here follows a detailed description of all the features (i.e. columns/variables) in the dataset.\n",
    "\n",
    "* **id** - unique identifier for a house\n",
    "* **date** - house was sold\n",
    "* **price** - price, our prediction target\n",
    "* **bedrooms** - number of Bedrooms/House\n",
    "* **bathrooms** - number of bedrooms\n",
    "* **sqft_living** - square footage of the home\n",
    "* **sqft_lot** - square footage of the entire lot\n",
    "* **floors** - total number of floors (levels) in house\n",
    "* **waterfront** - house which has a view to a waterfront\n",
    "* **view** - quality of view\n",
    "* **condition** - how good the condition is ( overall )\n",
    "* **grade** - overall grade given to the housing unit, based on King County grading system\n",
    "* **sqft_above** - square footage of house apart from basement\n",
    "* **sqft_basement** - square footage of the basement\n",
    "* **yr_built** - Built Year\n",
    "* **yr_renovated** - Year when house was renovated\n",
    "* **zipcode** - zip\n",
    "* **lat** - Latitude coordinate\n",
    "* **long** - Longitude coordinate\n",
    "* **sqft_living15** - The square footage of interior housing living space for the nearest 15 neighbours\n",
    "* **sqft_lot15** - The square footage of the land lots of the nearest 15 neighbours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "**Question:** which of these variables are quantitaive/numerical, ordinal, and categorical?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business problem\n",
    "\n",
    "We want to accurately predict the housing prices in the Kings county (Washington, US) based on the iformation available in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Training and Test Set\n",
    "\n",
    "Splitting the original dataset into training and test set should be done even before you start a more thorough Exploratory Data Analysis (EDA).\n",
    "\n",
    "Creating a test set is theoretically simple: pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside. We will use a function from `scikit-learn` which splits a dataset into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Stratified split\n",
    "\n",
    "Using `train_test_split()` we would just be doing a simple randomized sampling. But this might not be a representative sampling of the whole dataset, if we do not preserve the proportions (or percentages) of significant input features. Let's hypothesize that we learned from expert the `sqft_living` field is an important predictor for the house price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.sqft_living.hist(bins=100, figsize=(14,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to preserve the same proportion of samples with respect to `sqft_living` is to use a stratified split. We can split the dataset in such a way that the proportion of samples wrt `sqft_living` is preserved across the training and test set. To do this we first need to convert `sqft_living` into a categorical/ordinal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"sqft_living_cat\"] = pd.cut(\n",
    "    housing.sqft_living, \n",
    "    bins=[0., 1000., 2000., 3000., 4000., np.inf],\n",
    "    labels=[1, 2, 3, 4, 5]\n",
    ")\n",
    "housing['sqft_living_cat'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['sqft_living_cat'].value_counts() / len(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn offers the class `StratifiedShuffleSplit` to perform stratified splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in splitter.split(housing, housing.sqft_living_cat):\n",
    "    train_set = housing.loc[train_index]\n",
    "    test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the proportion of samples wrt `sqft_living_cat` is preserved in training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.sqft_living_cat.value_counts() / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.sqft_living_cat.value_counts() / len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now remove the `sqft_living_cat` feature as we will not need it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_ in (train_set, test_set):\n",
    "    set_.drop(\"sqft_living_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discover and Visualize the Data to Gain Insights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = train_set[\n",
    "    ['price', 'sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', \n",
    "     'lat', 'long', 'sqft_living15', 'sqft_lot15']\n",
    "].plot.box(subplots=True, layout=(3, 3), figsize=(18,18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_set[['price']].boxplot(figsize=(15,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot use the IQR method to display data and outliers(shape of the data) but in order to be get a list of identified outlier, we will need to use the mathematical formula and retrieve the outlier data.\n",
    "\n",
    "Wikipedia Definition:\n",
    "_The interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, $IQR = Q_3 − Q_1$.\n",
    "\n",
    "In other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data.\n",
    "\n",
    "It is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers._\n",
    "\n",
    "If a data point is below $Q_1 - 1.5\\times IQR$ or above $Q_3 + 1.5\\times IQR$ then it's an outlier.\n",
    "\n",
    "![box-plot-summary](../images/box-plot-iqr.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1 \n",
    "<b>Exercise 1:</b> Compute for me the count of outliers in our training set with respect to the `price` feature. (Hint: check the `DataFrame.quantile()` method and find a way to count the occurrences of values in a column of a DataFrame.) Additionally, write the code to remove those outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write you solution here. Add as many cells as you see fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the outliers legitimate or should we remove them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize geographical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.plot(\n",
    "    kind=\"scatter\", x=\"long\", y=\"lat\", figsize=(15,10)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.plot(kind=\"scatter\", x=\"long\", y=\"lat\", alpha=0.1, figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some info to the plot:\n",
    "    * scale the size of the markers based on the surface areas of the house\n",
    "    * colour-code the dots based on the house price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"long\",\n",
    "    y=\"lat\",\n",
    "    alpha=0.1,\n",
    "    figsize=(20,13),\n",
    "    s=train_set[\"sqft_living\"]/100,\n",
    "    label=\"sqft_living\",\n",
    "    c=\"price\",\n",
    "    cmap=plt.get_cmap('jet'),\n",
    "    colorbar=True\n",
    ")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same colormap (i.e. jet), we can try to improve the visualization above, setting an upper value that is reasonable, (i.e less or equal to QR3 + 1.5 IQR such as 1,000,000 $), and not the highest value in the range.\n",
    "\n",
    "We can create a custom discrete colorbar by using `matplotlib.colors.BoundaryNorm` as normalizer for your scatterplot. See the norm argument in `matplotlib.pyplot.scatter()`: https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.jet  # define the colormap\n",
    "bounds = np.linspace(0, q3.price + 1.5 * iqr.price, 11) # define 11 evenly space\n",
    "\n",
    "# The matplotlib.colors.BoundaryNorm class is used to create a colormap based on discrete numeric intervals.\n",
    "norm = mpl.colors.BoundaryNorm(\n",
    "    bounds, # Monotonically increasing sequence of boundaries\n",
    "    cmap.N # Number of colors in the colormap to be used\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 13))\n",
    "plt.scatter(\n",
    "    x=train_set[\"long\"],\n",
    "    y=train_set[\"lat\"],\n",
    "    alpha=0.1,\n",
    "    s=train_set[\"sqft_living\"]/100, # size of the dot\n",
    "    label=train_set[\"sqft_living\"],\n",
    "    c=train_set[\"price\"], # colour of the dot\n",
    "    cmap=cmap, # colour map \n",
    "    norm=norm # used to scale the color data, c, in the range 0 to 1, in order to map into the colormap cmap\n",
    ")\n",
    "plt.colorbar(label=\"Price\", orientation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to achieve the same result as above is to use a `matplotlib.colors.LinearSegmentedColormap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.jet  # define the colormap\n",
    "# extract all colors from the .jet map\n",
    "cmaplist = [cmap(i) for i in range(cmap.N)]\n",
    "# force the first color entry to be grey\n",
    "cmaplist[0] = (.5, .5, .5, 1.0)\n",
    "\n",
    "# create the new map\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    'Custom cmap', cmaplist, cmap.N\n",
    ")\n",
    "\n",
    "# define the bins and normalize\n",
    "bounds = np.linspace(0, q3.price + 1.5 * iqr.price, 11)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "plt.figure(figsize=(20, 13))\n",
    "plt.scatter(\n",
    "    x=train_set[\"long\"], y=train_set[\"lat\"],\n",
    "    alpha=0.1,\n",
    "    s=train_set[\"sqft_living\"]/100, label=train_set[\"sqft_living\"],\n",
    "    c=train_set[\"price\"], cmap=cmap, norm=norm\n",
    ")\n",
    "plt.colorbar(label=\"Price\", orientation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "**Question:** What insights can we infer from this graph?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "<b>Exercise 2:</b> explore on your own other ways to improve the graph above. You could look for ways to overlap it on top of the county map, or you could see if you can encode information differently.\n",
    "\n",
    "Map support can be provided using either [Basemap](https://basemaptutorial.readthedocs.io/en/latest/) or [folium](https://python-visualization.github.io/folium/).\n",
    "* To install basemap: `conda install -c conda-forge basemap basemap-data-hires`\n",
    "* To install folium: `conda install -c conda-forge folium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write here your possible solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Numerical features: looking for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is not that big, and we can compute the standard correlation coefficient (Pearson’s r coefficient) between every two features using the `DataFrame.corr()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_set.corr(numeric_only=True)\n",
    "corr_matrix[\"price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. The correlation coefficient only measures linear correlations, and it may completely miss nonlinear correlation factors. \n",
    "\n",
    "Another way to check for correlation visually is to use the `scatter_matrix()` utility function offered by Pandas, which leverages `matplotlib`, or seaborn's `pairplot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\n",
    "    \"price\", \"sqft_living\", \"grade\",\n",
    "    \"sqft_above\", \"sqft_living15\", \"bathrooms\"\n",
    "]\n",
    "pd.plotting.scatter_matrix(\n",
    "    train_set[attributes], figsize=(15, 10)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(train_set[attributes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Numerical features: checking for multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate copy of df without target variable (price), id, date and lat/lon to perform multicolinearity check\n",
    "train_set_pred = train_set.drop([\"id\", \"date\", \"lat\", \"long\"], axis=1)\n",
    "\n",
    "# check for multicolinearity with seaborn heatmap\n",
    "# compute the correlation matrix\n",
    "corr = round(train_set_pred.corr(numeric_only=True), 3)\n",
    "\n",
    "# set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "ax = sns.heatmap(corr, cmap=cmap, vmax=1, vmin=-1, center=0, square=True, linewidths=1, cbar_kws={\"shrink\": .75}, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the correlation matrix is simmetrical we can get rid of the upper triangle using a triangular boolean mask: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Let's visualise the mask with matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.matshow(mask, cmap=\"gray\")\n",
    "\n",
    "for (i, j), z in np.ndenumerate(mask):\n",
    "    ax.text(j, i, '{:0.0f}'.format(z), ha='center', va='center', c=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(15, 12))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "ax = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0, square=True, linewidths=1, cbar_kws={\"shrink\": .75}, annot=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question \n",
    "**Question:** what conclusions can we draw from this correlation matrix? Is there any redundant information in the dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Preparing numerical data for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['sqft_living','sqft_lot','sqft_above','sqft_basement','yr_built','sqft_living15','sqft_lot15']\n",
    "\n",
    "for feat in numerical_features:\n",
    "    sns.jointplot(\n",
    "        x=feat,\n",
    "        y=\"price\",\n",
    "        data=train_set,\n",
    "        kind='reg',\n",
    "        label=feat,\n",
    "        joint_kws={'line_kws':{'color':'orange'}}\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already seen in the multicolinearity test `sqft_living`, `sqft_living15`, and `sqft_above` predictors all seem to have prominent linear correlation with price, Of the three, we will use `sqft_living` as it is the one that mostly correlates with price. \n",
    "\n",
    "We can notice that there are a bunch of zeros in `sqft_basement`. That indicates that those homes do not have basements.To tackle that, we will create a binary feature that indicates whethere a house has a basement or not.\n",
    "\n",
    "Finally, `yr_built` does not seem to show any clear relationship with price. We could combine the information of `yr_built` with `yr_renovated` to see get a more useful information. We will create a `renovated` binary flag. If a house is older than 25 years (relative to the most recent data in the dataset) and has not been renovated we will set `renovated` to 0, otherwise to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Categorical and Ordinal Features\n",
    "\n",
    "Let's examine the categorical and ordinal features more closely. Boxplots and violin plots are the most suited type of graphs for plotting a categorical variable against a numerical one (i.e. our target variable `price`)\n",
    "\n",
    "First we need to convert these columns to the category type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['bedrooms','bathrooms','floors','waterfront','view','condition','grade','zipcode']\n",
    "train_set[categorical_features] = train_set[categorical_features].astype('category')\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then start using boxplots or violinplots to further investigate targeted correlations, such as 'grade' vs 'price' or 'floors' vs 'price'.\n",
    "\n",
    "## Exercise 3\n",
    "\n",
    "<b>Exercise 3:</b> write a function that takes a categorical or ordinal feature as a first argument, the size of a figure as a second argument and plots, using seaborn, a set of boxplots of the price distribution for each category in the input categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_boxplot(\n",
    "    data_set: pd.DataFrame,\n",
    "    feature: str, \n",
    "    figsize: tuple[int, int] = (14, 6), \n",
    "    xlabels_rotation: int = None\n",
    "):\n",
    "    \"\"\"\n",
    "    data_set: the dataset with the feature and price columns\n",
    "    feature: the name of the feature to plot against \"price\"\n",
    "    figsize: the (width, height) size of the figure\n",
    "    xlabels_rotation: the rotation of the X labels. Default is equivalent to no rotation\n",
    "    \"\"\"\n",
    "    # create a dataframe containing only feature and \"price\"\n",
    "    data = ...\n",
    "    # specify the size of the figure\n",
    "    ...\n",
    "    # create the feature vs price boxplot using seaborn. Check the seaborn boxplot documentation\n",
    "    chart = ...\n",
    "    # if an x-label rotation is provided, rotate the x label of the chart\n",
    "    if xlabels_rotation is not None:\n",
    "        chart.set_xticklabels(chart.get_xticklabels(), rotation=xlabels_rotation, horizontalalignment='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try your function to plot 'grade' vs 'price'.\n",
    "## If you have implemented it correctly it will plot out the boxplots\n",
    "print_boxplot(train_set, 'grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try your function to plot 'floors' vs 'price'.\n",
    "## If you have implemented it correctly it will plot out the boxplots\n",
    "print_boxplot(train_set, 'floors', figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try your function to plot 'bathrooms' vs 'price'.\n",
    "## If you have implemented it correctly it will plot out the boxplots\n",
    "print_boxplot(train_set, 'bathrooms', figsize=(20, 12))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question \n",
    "**Question:** Do you notice anything suspicious with bathrooms?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_boxplot(train_set, 'bedrooms', figsize=(20, 12))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question \n",
    "**Question:** Do you notice anything suspicious with bedrooms?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_boxplot(train_set, \"view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_boxplot(train_set, \"waterfront\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_boxplot(train_set, \"zipcode\", figsize=(24, 8), xlabels_rotation= 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Categorical variables: statistical test to evaluate significance (optional)\n",
    "\n",
    "#### ANOVA test for the \"view\" feature\n",
    "\n",
    "Analysis of Variance (ANOVA) is a statistical test that analyses the difference among means. ANOVA checks if the means of two or more groups are significantly different from each other. In our case we want to see if the average prices are significantly different among the 5 view categories.\n",
    "\n",
    "We can compute ANOVA using the `statsmodels` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod = ols(\"price ~ view\", train_set).fit()\n",
    "table = sm.stats.anova_lm(lin_mod, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the p-value for view is 0.000 < 0.05, the difference of average house prices among the view categories is significant.\n",
    "\n",
    "#### Post-hoc test\n",
    "\n",
    "We can run a post-hoc test to make pairwise comparisons and see which pairs of view categories have significant mean differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_t = lin_mod.t_test_pairwise(\"view\")\n",
    "pair_t.result_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the differences are significant except between view 1 and view 2. This could already be discerned from the boxplot graph. We could then rework the view categories merging together views 1 and 2 and reducing the 5 categories to 4.\n",
    "\n",
    "Another option would be convert this feature to a binary one (\"view\" vs \"no view\"). For our model training we will keep all the view levels and treat it as an ordinal variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA and post-hoc tests on ZIPCODE\n",
    "\n",
    "We could do a symilar analysis on the `zipcode` field to reduce the number of categories there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodes = train_set[\"zipcode\"].unique()\n",
    "zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 70 zipcodes. That is a bit too many categories given the number of samples we have. A plurality of these zipcodes likely have very few samples so we may encur into overfitting our models. We could find a way to cluster/collapse these zipcodes together, based on whether\n",
    "their average house price is significantly different or not.\n",
    "To do this we can first do an ANOVA test on `zipcode` and their peform pairwise post-hoc t-test and then collapse together those zipcodes whose t-test indicates that the means are not significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod = ols(\"price ~ zipcode\", train_set).fit()\n",
    "table = sm.stats.anova_lm(lin_mod, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the p-value for view is 0.000 < 0.05, the difference of average house prices among the view categories is significant.\n",
    "\n",
    "#### Post-hoc test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_t = lin_mod.t_test_pairwise(\"zipcode\")\n",
    "pair_t.result_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now extract the zipcode pairs whose post-hoc t-test fails to reject the null hypothesis (i.e. the average prices are not significantly different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests_frame = pair_t.result_frame[pair_t.result_frame[\"reject-hs\"].isin([False])].reset_index(names=\"zipcode-pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests_frame[\"zipcode-pairs\"] = ttests_frame[\"zipcode-pairs\"].str.split(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttests_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll group together all the zipcodes for which the mean house price is not significantly different. In doing so, we will create a new category named `zipcode_group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_zipcode_groups(data_set: pd.DataFrame) -> list[set]:\n",
    "    \"\"\"\n",
    "    Groups together zipcodes with similar house prices\n",
    "    :param data_set: it can be the training or test set\n",
    "    :returns: he generated zipcode groups. Each zipcode group is a set\n",
    "    \"\"\"\n",
    "    # firsr we perform a post-hoc t-test with correction for each zipcode pair\n",
    "    pair_t = ols(\"price ~ zipcode\", train_set).fit().t_test_pairwise(\"zipcode\")\n",
    "    # we retain all the zipcode pairs for which the post-hoc test fails to reject the null hypothesis\n",
    "    # these are all the zipcode pairs whose average house price is not significantly different with a 95% CI\n",
    "    t_tests_frame = pair_t.result_frame[pair_t.result_frame[\"reject-hs\"].isin([False])].reset_index(names=\"zipcode-pairs\")\n",
    "    # we extract the zipcode pairs as a list\n",
    "    zipcode_pairs = t_tests_frame[\"zipcode-pairs\"].str.split(\"-\").tolist()\n",
    "    # we group together zipcodes with \"similar\" average house prices\n",
    "    zipcode_groups = []\n",
    "    for zipcode in train_set[\"zipcode\"].unique():\n",
    "        if any(zipcode in group for group in zipcode_groups):\n",
    "            continue\n",
    "        new_group = { zipcode }\n",
    "        for pair in zipcode_pairs:\n",
    "            if zipcode in pair:\n",
    "                new_group.update(pair)\n",
    "        zipcode_groups.append(new_group)\n",
    "    return zipcode_groups "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "**Exercise 4:** Let's test the function `make_zipcode_groups()` on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced our 70 zipcodes to just 9 zipcode groups. This should not impact too much our predictive power wrt `price` while reducing the risk of overfitting.\n",
    "\n",
    "We can now assign each sample in the training and test set a `zipcode_group` feature based on its zipcode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_zipcode_group(zipcode: str, zipcode_groups: list[set], group_prefix=\"zg\") -> str:\n",
    "    \"\"\"\n",
    "    given a zipcode and a list of groups it assign a zipcode to a group\n",
    "    The groups will be called using the group_prefix as a prefix\n",
    "    \"\"\"\n",
    "    try:\n",
    "        res = next(i for i, group in enumerate(zipcode_groups) if zipcode in group)\n",
    "        return f\"{group_prefix}_{res}\"\n",
    "    except StopIteration:\n",
    "        # we need to take into account that some zipcode may be missing in the training set and present in the test set\n",
    "        return f\"{group_prefix}_other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"zipcode_group\"] = train_set[\"zipcode\"].apply(assign_to_zipcode_group, zipcode_groups=zipcode_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"zipcode_group\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"zipcode_group\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: as an expected consequence of our clustering, some zipcode groups are now over-represented in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[\"zipcode_group\"] = test_set[\"zipcode\"].apply(assign_to_zipcode_group, zipcode_groups=zipcode_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"zipcode_group\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out all the zipcodes were present in the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusions of the EDA:\n",
    "\n",
    "At the end of our EDA we have reached a few conclusions:\n",
    "\n",
    "* We have identified quantitative, ordinal and categorical variables.\n",
    "* We have identified a cutoff at latitude  ~47.5 between more expensive houses and cheaper houses.We will create a binary engineered feature to capture this. We could also create a cutoff at -126.1 long to separate the urban west from the rural east of the county. We will then remove `lat` and `long`.\n",
    "* We have decided to discard `sqft_living15` and `sqft_living_above` in favour of `sqft_living`\n",
    "* We have decided to add a binary engineered feature that indicates whether a house has a basement or not. We will then remove the continuous variable `sqft_basement`\n",
    "* We will create a renovated binary flag. If a house is older than 25 years (relative to the most recent data in the dataset) and has not been renovated we will set renovated to 0, otherwise to 1. We will then remove the continuous variable `yr_built` and `yr__renovated`\n",
    "* We have decided to collapse the 70 zipcodes into 9 zipcode groups based on average house prices in the zipcodes \n",
    "* Some houses report 0 bathrooms. We need to replace those values with more meaningful estimates.\n",
    "* One house has 33 bedrooms. We will replace that value with 3, as it looks like a reporting mistake."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4adc19297ff67e92365d65fbb293b9763c71c243813490754e9fef78d503e715"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
